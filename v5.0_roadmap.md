# SRA Roadmap v5.0: The Local MLOps Evolution

This document is the **Master Technical Specification** for the SRA v5.0 evolution. It serves as both a public-facing roadmap and an internal engineering blueprint for transitioning SRA from a cloud-dependent architecture (Gemini) to a completely **Sovereign, Local MLOps Ecosystem**.

> **Last Updated**: 2026-02-21

---

## ðŸ›ï¸ Pillar 1: Sovereign Inference Engine (Local LLM Integration)
**Goal**: Completely replace Google Gemini APIs with self-hosted open-weights models without sacrificing output quality.

### Current State (v4.x â€” Cloud)
*   `aiService.js` supports **two providers**: `google` (Gemini 2.5 Flash â€” default) and `openai` (OpenAI-compatible API).
*   `BaseAgent.js` â€” all 5 agents (Developer, Reviewer, Critic, ProductOwner, Architect) are hardcoded to `genAI` (Gemini SDK).
*   `repairDiagram()` is locked to `genAI.getGenerativeModel()`.

### v5.0 Target
1.  **Local Model Server**: Deploy an inference server (e.g., **Ollama**, **vLLM**, or **LM Studio**) to serve models locally and expose an OpenAI-compatible API.
2.  **Model Selection**: Switch from `gemini-2.5-flash` to comparable open-source models:
    *   *Primary Reasoning:* `Qwen2.5-7B-Instruct` (default in `train_sra.py`), `Llama-3-8B-Instruct`, or `Mistral-Nemo`.
    *   *Agentic Reflection/Reviewing:* Larger variants or quantized models (GGUF) if hardware permits.
3.  **The Adapter Layer**: Add a `local` model provider to `aiService.js` that routes API calls to the local inference server. Refactor `BaseAgent.js` to accept provider config instead of hardcoding `genAI`.

### Migration Checklist
- [ ] Add `local` provider branch in `aiService.js` (OpenAI-compatible client â†’ vLLM/Ollama)
- [ ] Refactor `BaseAgent.js` to accept a provider parameter instead of importing `genAI` directly
- [ ] Update `repairDiagram()` to route through the adapter layer
- [ ] Test all 5 agents (Developer, Reviewer, Critic, ProductOwner, Architect) against local models

---

## ðŸ”— Pillar 2: Local AI Agents & Deterministic JSON
**Goal**: Ensure the multi-agent reflection loop (Developer, Reviewer, Critic) operates reliably on local hardware.

### Current State (âœ… Partially Ready)
*   **5-Agent Architecture** operational: `DeveloperAgent`, `ReviewerAgent`, `CriticAgent`, `ProductOwnerAgent`, `ArchitectAgent` â€” all extend `BaseAgent`.
*   **JSON Repair Pipeline**: `BaseAgent.parseJSON()` uses `jsonrepair` library + markdown stripping + brace extraction + trailing comma removal.
*   **Zod Schema Validation**: `aiService.js` validates all outputs against `AnalysisResultSchema` with nested field fallback recovery (`srs`, `result` keys).
*   **JSON Mode**: Gemini's `responseMimeType: "application/json"` is already active.

### v5.0 Target
*   **Structured Outputs / JSON Mode**: Local models often drift from strict schemas. Integrate robust grammar engines (like `outlines`, Ollama JSON mode, or vLLM structured generation) to ensure `AnalysisResultSchema` is perfectly adhered to.
*   **Prompt Refactoring**: SRA's `masterPrompt` and `DIAGRAM_REPAIR_PROMPT` will be benchmarked and rewritten specifically for Llama/Qwen instruction formats.
*   **Hardened Parsing**: The existing `jsonrepair` pipeline in `BaseAgent.parseJSON()` is a strong foundation. Add structured output constraints on the model side to reduce repair frequency.

---

## ðŸ¤ Pillar 3: Private RAG & Offline Document Intelligence
**Goal**: Transition vector embeddings and RAG from cloud dependency to 100% offline document retrieval.

### Current State (âœ… Functional â€” Cloud-Bound)
*   **Embedding Model**: `embeddingService.js` uses `gemini-embedding-001` (Google Cloud, 768 dimensions).
*   **Vector Store**: pgvector extension via Prisma raw queries. Cosine similarity search with aggressive Gold Standard prioritization (`qualityScore >= 0.85`).
*   **Graph RAG**: `ragService.js` combines vector search with `graphService.js` entity traversal for relationship-aware context retrieval.
*   **RAG Evaluation**: `evalService.js` implements RAGAS-style scoring (Faithfulness, Context Precision, Answer Relevancy) using an LLM-as-judge pattern.
*   **Context Injection**: `aiService.js` automatically injects RAG context into the system prompt with truncation caps (~8000 chars).

### v5.0 Target
*   **Local Embedding Models**: Replace `gemini-embedding-001` in `embeddingService.js` with local equivalents (e.g., `nomic-embed-text` or `bge-m3` via Ollama).
*   **Vector Database Optimization**: Ensure pgvector is containerized and bound to the local network to maintain zero-data-leakage.
*   **Dimension Alignment**: Verify embedding dimensions match (current: 768) when switching models, or re-embed all existing knowledge chunks.

### Migration Checklist
- [ ] Swap `gemini-embedding-001` â†’ local model in `embeddingService.js`
- [ ] Re-embed existing `KnowledgeChunk` records with new model
- [ ] Validate Graph RAG + vector search still works end-to-end
- [ ] Update `evalService.js` to use local model for judge evaluations

---

## ðŸ›¡ï¸ Pillar 4: The Data Flywheel & High-Performance Fine-Tuning
**Goal**: Leverage college NVIDIA lab hardware to train a bespoke "SRA-Pro" model that natively handles 20k+ token outputs with zero "lazy" summarization.

### Data Pipeline (âœ… Operational)
*   **Document Harvesting**: `data_harvester.py` extracts raw text from **79 SRS documents** (PDF, HTML, DOCX, legacy DOC via COM automation) into `pure_raw_dataset.json` (6.7 MB).
*   **SFT Formatting**: Pipeline converts raw documents into instruction/input/output JSONL format â†’ **70 training samples**, **9 test samples**.
*   **Token Analysis**: Average sample is **~32,260 tokens**. Distribution: 84.3% > 16k tokens, 28.6% > 32k tokens, 5.7% > 64k tokens.
*   **Quality Filtering**: Samples exceeding **65,536 tokens** are automatically pruned â†’ **66 training samples** retained (4 removed), all 9 test samples kept.
*   **Dataset Verification**: `verify_dataset_quality.js` validates JSON structure and Mermaid diagram integrity; `deep_mermaid_audit.js` performs syntax-level checks.

### Training Strategy (Enterprise/Lab Grade)
*   **Base Model Selection**: Default is **Qwen2.5-7B-Instruct**, with support for DeepSeek, Llama, GLM, and Kimi variants.
*   **QLoRA Fine-Tuning**: 4-bit NF4 quantization with double quantization, LoRA rank 64, alpha 128, targeting all linear layers. Flash Attention 2 enabled.
*   **Max Sequence Length**: **65,536 tokens** â€” matching the filtering ceiling to eliminate truncation.
*   **JSON-First Objective**: Train specifically on loss functions that penalize invalid JSON syntax or broken Mermaid diagrams, ensuring the local model is more reliable than Gemini for our specific schema.
*   **Training Setup**: `train_sra.py` uses **HuggingFace TRL SFTTrainer** + **PEFT** + **BitsAndBytes** with WandB logging. Ready for multi-GPU deployment via DeepSpeed.

### Pipeline Scripts (13 total)
| Script | Purpose |
| :--- | :--- |
| `data_harvester.py` | Extract text from 79 SRS docs (PDF/HTML/DOCX/DOC) |
| `merge_and_format_dataset.py` | Merge raw JSON â†’ split into train/test JSONL |
| `harmonize_dataset.js` | Normalize and clean dataset entries |
| `split_dataset.py` / `.js` | Train/test splitting utilities |
| `analyze_dataset_tokens.py` | Token distribution analysis |
| `filter_long_samples.py` | Remove samples exceeding 65k tokens |
| `verify_dataset_quality.js` | JSON + Mermaid structural validation |
| `deep_mermaid_audit.js` | Syntax-level Mermaid diagram checks |
| `repair_dataset.js` | Auto-repair broken entries |
| `train_sra.py` | Full QLoRA training script (lab-ready) |

---

## ðŸ“ˆ Pillar 5: The MLOps Resource & Observability Layer
**Goal**: Orchestrate local vs. Lab hardware and monitor inference health.

### Current State (âœ… Partially Implemented)
*   **Job Queue**: `queueService.js` uses **Upstash QStash** (cloud) for async analysis jobs with idempotency checks. Has `MOCK_QSTASH` fallback for local dev (processes jobs in-process).
*   **Redis**: `redis.js` configured with **ioredis** + Upstash TLS. Used for rate limiting & caching.
*   **Docker Compose**: Production-ready with **Nginx reverse proxy** (load balancer), **2 backend replicas** (scalable), resource limits (512MB/backend, 1GB/frontend), health checks, and a CLI tools container.
*   **RAG Evaluation**: `evalService.js` runs automated RAGAS-style quality checks after each generation.
*   **Logging**: Structured JSON logging via Pino (`logger.js`).

### v5.0 Target
*   **Hardware Orchestration**: Create specialized `docker-compose.gpu.yml` for local GPU testing and Slurm/Remote scripts for college lab deployment.
*   **Queue Migration**: Replace Upstash QStash with **BullMQ** (Redis-based) for fully local, self-hosted job processing without cloud dependency.
*   **Tracing & Rate Limiting**: Implement VRAM-aware rate limiting to handle heavy multi-agent reflection loops without crashing the GPU.
*   **Evaluations (CI/CD for AI)**: Automated benchmarking of every new model checkpoint against the "6Cs" quality metric (Clarity, Completeness, Conciseness, Consistency, Correctness, Context) before pushing to SRA production.

### Migration Checklist
- [ ] Create `docker-compose.gpu.yml` with NVIDIA runtime, vLLM container, and GPU resource limits
- [ ] Replace `queueService.js` Upstash QStash â†’ BullMQ + local Redis
- [ ] Add VRAM monitoring and GPU-aware concurrency limits
- [ ] Integrate 6Cs automated benchmark into CI/CD pipeline
- [ ] Create Slurm job scripts for college lab batch training

---

## ðŸŒ The Sovereign Architecture

| Component | v4.0 (Cloud) | v5.0 (Sovereign Local) |
| :--- | :--- | :--- |
| **Brain / Inference** | Gemini 2.5 Flash | Qwen2.5-7B-Instruct (default) |
| **Customization** | Massive Prompt Prefixes | QLoRA Fine-Tuned "SRA-Pro" Adapters |
| **Inference Engine** | Google Cloud | vLLM / TensorRT-LLM (Lab hosted) |
| **Embeddings** | `gemini-embedding-001` (Cloud) | `nomic-embed-text` / `bge-m3` (Local) |
| **Vector Store** | pgvector (Cloud DB) | pgvector (Containerized, Air-Gapped) |
| **Job Queue** | Upstash QStash (Cloud) | BullMQ + Redis (Self-Hosted) |
| **Data Privacy** | Cloud Agreement | 100% Air-Gapped / College Lab |
| **Training Data** | N/A | 79 SRS docs â†’ 66 filtered SFT samples |
| **Agent Framework** | 5 Agents (Gemini-bound) | 5 Agents (Provider-agnostic) |

---

## ðŸ“… Target Implementation Schedule

| Phase | Milestone | Status | Engineering Focus |
| :--- | :--- | :---: | :--- |
| **Phase 1** | **The Adapter** | â¬œ | Add `local` provider to `aiService.js` + refactor `BaseAgent.js` for provider-agnostic routing. |
| **Phase 2** | **The Grammar** | â¬œ | Implement structured output constraints / grammar for local model JSON compliance. |
| **Phase 3** | **The Private Node** | â¬œ | Swap `embeddingService.js` to local models, re-embed knowledge chunks, containerize pgvector. |
| **Phase 4** | **The Lab Training** | ðŸŸ¡ | Data pipeline complete. Training script ready. Pending first fine-tuning run on lab hardware. |
| **Phase 5** | **The Queue** | â¬œ | Migrate `queueService.js` from Upstash QStash to BullMQ. Create `docker-compose.gpu.yml`. |
| **Phase 6** | **The Switchover** | â¬œ | Cut Gemini access completely. Route all generation + embedding to SRA-Pro local stack. |

---

## ðŸ”¬ Current Progress Snapshot

| Metric | Value |
| :--- | :--- |
| Documents Harvested | 79 (PDF, HTML, DOCX, DOC) |
| Raw Dataset Size | 6.7 MB (`pure_raw_dataset.json`) |
| SFT Training Samples | 66 (after 65k token filter) |
| SFT Test Samples | 9 |
| Avg Token Length | ~32,260 tokens/sample |
| Max Token Length | ~139,342 tokens (pre-filter) |
| Training Script | `train_sra.py` â€” ready for lab deployment |
| Pipeline Scripts | 13 (harvest, split, format, analyze, filter, verify, repair) |
| Active Agents | 5 (Developer, Reviewer, Critic, ProductOwner, Architect) |
| Backend Services | 21 (AI, RAG, Embedding, Queue, Eval, Graph, Security, etc.) |
| Docker Services | 4 (Nginx LB, Backend Ã—2, Frontend, CLI) |

---

> [!CAUTION]
> SRA v5.0 introduces extreme hardware constraints compared to cloud APIs. Concurrency limits, prompt lengths, and strict schema validations must be handled via intermediate backend logic rather than relying on the LLM's inherent capabilities. The `BaseAgent.js` and `aiService.js` adapter refactoring is the critical-path dependency for all other pillars.
